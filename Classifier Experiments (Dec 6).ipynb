{
 "metadata": {
  "name": "",
  "signature": "sha256:78842cd5e8e2e308315f29f57101bedcd23f34d82c003373241beb5427a8c687"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys, os, re, json\n",
      "import collections\n",
      "%pylab inline\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import data_utils\n",
      "reload(data_utils)\n",
      "\n",
      "import analysis\n",
      "reload(analysis)\n",
      "\n",
      "import classifier\n",
      "reload(classifier)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "<module 'classifier' from 'classifier.py'>"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load data and generate features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "infile = \"data/mturk-prod.train\"\n",
      "df = data_utils.load_annotated(infile)\n",
      "\n",
      "# Restrict to unambiguous labels\n",
      "nunamb = len(df[df.__LABEL__.notnull()])\n",
      "print \"%d unambiguous labels\" % nunamb\n",
      "nsentence = len(df[df.__LABEL_BIN__ == \"-SENTENCE-\"])\n",
      "print \"%d sentences (%.02f%%)\" % (nsentence, 100*nsentence/(1.0*nunamb))\n",
      "\n",
      "df = data_utils.make_basic_features(df)\n",
      "print df.shape\n",
      "for c in df.columns:\n",
      "    print c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5306 unambiguous labels\n",
        "1884 sentences (35.51%)\n",
        "(9000, 17)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "__LABEL__\n",
        "__TEXT__\n",
        "lemma\n",
        "ner\n",
        "pos\n",
        "word\n",
        "__LABELS__\n",
        "__LABEL_BIN__\n",
        "f_nchars\n",
        "f_nwords\n",
        "f_npunct\n",
        "f_rpunct\n",
        "f_ndigit\n",
        "f_rdigit\n",
        "f_nner\n",
        "f_rner\n",
        "f_sentence_pattern\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate POS distributional features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pdf = data_utils.get_pos_counts(df)\n",
      "# display(pdf.head())\n",
      "print pdf.columns\n",
      "\n",
      "# Normalize by row\n",
      "pdf_norm = pdf.divide(pdf.sum(axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Index([u'f_pos_#', u'f_pos_$', u'f_pos_''', u'f_pos_,', u'f_pos_-LRB-', u'f_pos_-RRB-', u'f_pos_.', u'f_pos_:', u'f_pos_CC', u'f_pos_CD', u'f_pos_DT', u'f_pos_EX', u'f_pos_FW', u'f_pos_IN', u'f_pos_JJ', u'f_pos_JJR', u'f_pos_JJS', u'f_pos_LS', u'f_pos_MD', u'f_pos_NN', u'f_pos_NNP', u'f_pos_NNPS', u'f_pos_NNS', u'f_pos_PDT', u'f_pos_POS', u'f_pos_PRP', u'f_pos_PRP$', u'f_pos_RB', u'f_pos_RBR', u'f_pos_RBS', u'f_pos_RP', u'f_pos_SYM', u'f_pos_TO', u'f_pos_UH', u'f_pos_VB', u'f_pos_VBD', u'f_pos_VBG', u'f_pos_VBN', u'f_pos_VBP', u'f_pos_VBZ', u'f_pos_WDT', u'f_pos_WP', u'f_pos_WP$', u'f_pos_WRB', u'f_pos_``'], dtype='object')\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Classifier Experiments\n",
      "Convert to sklearn format, and run preprocessing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CV_N_JOBS = 6\n",
      "\n",
      "# Merge -> all features\n",
      "data = df.merge(pdf_norm, how='outer', left_index=True, right_index=True)\n",
      "\n",
      "label_col = \"__LABEL_BIN__\"\n",
      "data = data[data[label_col].notnull()]\n",
      "X, y, int_to_label, col_to_feature = data_utils.dataframe_to_xy(data, \n",
      "                                                                r\"f_.*\", \n",
      "                                                                label_col=label_col)\n",
      "from sklearn import preprocessing\n",
      "X = preprocessing.StandardScaler().fit_transform(X)\n",
      "\n",
      "print X.shape\n",
      "print int_to_label\n",
      "print \"Features: \" + \", \".join(col_to_feature.values())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'df' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-b5c734f09a00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Merge -> all features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'outer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlabel_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"__LABEL_BIN__\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Test Classifiers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import classifier\n",
      "reload(classifier)\n",
      "\n",
      "# Logistic experiment\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "clf = LogisticRegression(penalty='l1', C=1000, intercept_scaling=1, \n",
      "                         class_weight='auto')\n",
      "param_grid = [{'C': [0.1,1,10,100,1000], \n",
      "               'penalty':['l1', 'l2']}]\n",
      "\n",
      "exp = classifier.ClassifierExperiment(clf, X, y)\n",
      "exp.grid_search(param_grid, scoring='f1')\n",
      "res, stats = exp.eval_cv(eval_train=True)\n",
      "\n",
      "print \"Best features:\"\n",
      "for l,w in sorted(zip(col_to_feature.values(), exp.clfopt.best_estimator_.coef_[0]),\n",
      "                  key=lambda s: abs(s[1]), reverse=True)[:10]:\n",
      "    print \"%s : %.04f\" % (l.ljust(10),w) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best params: {'penalty': 'l2', 'C': 1}\n",
        "Best score: 92.14%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "== train sets ==\n",
        "5-fold cv:\n",
        " acc: 94.71 +\\- 0.12%\n",
        " pre: 91.26 +\\- 0.17%\n",
        " rec: 94.12 +\\- 0.17%\n",
        "  f1: 92.67 +\\- 0.17%\n",
        "== test sets ==\n",
        "5-fold cv:\n",
        " acc: 94.31 +\\- 0.36%\n",
        " pre: 90.46 +\\- 0.82%\n",
        " rec: 93.90 +\\- 0.51%\n",
        "  f1: 92.14 +\\- 0.48%\n",
        "Best features:\n",
        "f_sentence_pattern : 1.8052\n",
        "f_npunct   : -1.2783\n",
        "f_pos_:    : -0.9162\n",
        "f_pos_.    : 0.8866\n",
        "f_nchars   : -0.7720\n",
        "f_pos_VB   : 0.6718\n",
        "f_ndigit   : -0.5694\n",
        "f_pos_VBZ  : 0.5497\n",
        "f_pos_VBP  : 0.5470\n",
        "f_nner     : 0.3948\n"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import classifier\n",
      "reload(classifier)\n",
      "\n",
      "# Linear SVM Experiment\n",
      "from sklearn.svm import LinearSVC\n",
      "clf = LinearSVC(penalty='l1', C=1000, intercept_scaling=1, \n",
      "                class_weight='auto', dual=False)\n",
      "param_grid = [{'C': [0.001, 0.01, 0.1,1,10,100,1000], \n",
      "               'penalty':['l1', 'l2']}]\n",
      "\n",
      "exp = classifier.ClassifierExperiment(clf, X, y)\n",
      "exp.grid_search(param_grid, scoring='f1')\n",
      "res, stats = exp.eval_cv(eval_train=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best params: {'penalty': 'l2', 'C': 0.01}\n",
        "Best score: 92.16%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "== train sets ==\n",
        "5-fold cv:\n",
        " acc: 94.67 +\\- 0.07%\n",
        " pre: 91.38 +\\- 0.12%\n",
        " rec: 93.84 +\\- 0.15%\n",
        "  f1: 92.60 +\\- 0.11%\n",
        "== test sets ==\n",
        "5-fold cv:\n",
        " acc: 94.35 +\\- 0.26%\n",
        " pre: 90.84 +\\- 0.63%\n",
        " rec: 93.52 +\\- 0.57%\n",
        "  f1: 92.16 +\\- 0.35%\n"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import classifier\n",
      "reload(classifier)\n",
      "\n",
      "# Non-linear SVM Experiment\n",
      "from sklearn.svm import SVC\n",
      "clf = SVC(C=1000, class_weight='auto')\n",
      "param_grid = [{'C': [0.1,1,10,100,1000],\n",
      "               'gamma': [0.0, 0.001, 0.01, 0.1],\n",
      "               'kernel': ['rbf']}]\n",
      "\n",
      "exp = classifier.ClassifierExperiment(clf, X, y)\n",
      "exp.grid_search(param_grid, scoring='f1')\n",
      "res, stats = exp.eval_cv(eval_train=True)\n",
      "\n",
      "# Check support vector size\n",
      "exp.clf.fit(X,y)\n",
      "exp.clf.n_support_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best params: {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
        "Best score: 92.38%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "== train sets ==\n",
        "5-fold cv:\n",
        " acc: 95.75 +\\- 0.10%\n",
        " pre: 93.38 +\\- 0.23%\n",
        " rec: 94.76 +\\- 0.11%\n",
        "  f1: 94.07 +\\- 0.14%\n",
        "== test sets ==\n",
        "5-fold cv:\n",
        " acc: 94.55 +\\- 0.28%\n",
        " pre: 91.78 +\\- 0.42%\n",
        " rec: 92.99 +\\- 0.60%\n",
        "  f1: 92.38 +\\- 0.39%\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 193,
       "text": [
        "array([481, 309], dtype=int32)"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import classifier\n",
      "reload(classifier)\n",
      "\n",
      "# Random Forest Experiment\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "clf = RandomForestClassifier(n_estimators=10, max_depth=10)\n",
      "param_grid = [{'n_estimators': [10,50,100,200],\n",
      "               'max_depth': [5,10,20,40]}]\n",
      "\n",
      "exp = classifier.ClassifierExperiment(clf, X, y)\n",
      "exp.grid_search(param_grid, scoring='f1')\n",
      "res, stats = exp.eval_cv(eval_train=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best params: {'n_estimators': 100, 'max_depth': 40}\n",
        "Best score: 93.64%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "== train sets ==\n",
        "5-fold cv:\n",
        " acc: 99.98 +\\- 0.01%\n",
        " pre: 100.00 +\\- 0.00%\n",
        " rec: 99.95 +\\- 0.01%\n",
        "  f1: 99.97 +\\- 0.01%\n",
        "== test sets ==\n",
        "5-fold cv:\n",
        " acc: 95.34 +\\- 0.29%\n",
        " pre: 94.90 +\\- 0.30%\n",
        " rec: 91.83 +\\- 0.73%\n",
        "  f1: 93.33 +\\- 0.43%\n"
       ]
      }
     ],
     "prompt_number": 194
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import classifier\n",
      "reload(classifier)\n",
      "\n",
      "# Boosting Experiment\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "clf = AdaBoostClassifier(n_estimators=10)\n",
      "param_grid = [{'n_estimators': [10,50,100,200]}]\n",
      "\n",
      "exp = classifier.ClassifierExperiment(clf, X, y)\n",
      "exp.grid_search(param_grid, scoring='f1')\n",
      "res, stats = exp.eval_cv(eval_train=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best params: {'n_estimators': 50}\n",
        "Best score: 92.44%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "== train sets ==\n",
        "5-fold cv:\n",
        " acc: 95.22 +\\- 0.07%\n",
        " pre: 93.32 +\\- 0.12%\n",
        " rec: 93.21 +\\- 0.13%\n",
        "  f1: 93.26 +\\- 0.10%\n",
        "== test sets ==\n",
        "5-fold cv:\n",
        " acc: 94.63 +\\- 0.20%\n",
        " pre: 92.48 +\\- 0.66%\n",
        " rec: 92.41 +\\- 0.30%\n",
        "  f1: 92.44 +\\- 0.26%\n"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}